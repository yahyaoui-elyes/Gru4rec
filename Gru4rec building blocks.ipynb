{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "incident-armenia",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-22T20:15:19.469369Z",
     "start_time": "2021-06-22T20:14:56.825151Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-59f0177af1a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.models import Model,Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.layers import Input, Dense, Dropout, GRU, Embedding\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-queensland",
   "metadata": {},
   "source": [
    "# Session data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-advance",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-22T20:15:19.496051Z",
     "start_time": "2021-06-22T20:14:58.777Z"
    }
   },
   "outputs": [],
   "source": [
    "class SessionDataset:   \n",
    "    def __init__(self, data, session_key='SESSIONID', item_key='NAVIGATIONCODE', time_key='TIMESTAMP', n_samples=-1, itemmap=None, time_sort=False) :\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            path: path of the csv file\n",
    "            sep: separator for the csv\n",
    "            session_key, item_key, time_key: name of the fields corresponding to the sessions, items, time\n",
    "            n_samples: the number of samples to use. If -1, use the whole dataset.\n",
    "            itemmap: mapping between item IDs and item indices\n",
    "            time_sort: whether to sort the sessions by time or not\n",
    "        \"\"\"\n",
    "        self.df = data\n",
    "        self.session_key = session_key\n",
    "        self.item_key = item_key\n",
    "        self.time_key = time_key\n",
    "        self.time_sort = time_sort\n",
    "        self.add_item_indices(itemmap=itemmap)\n",
    "        self.df.sort_values([self.session_key, self.time_key], inplace=True)\n",
    "\n",
    "        # Sort the df by time, and then by session ID. That is, df is sorted by session ID and\n",
    "        # clicks within a session are next to each other, where the clicks within a session are time-ordered.\n",
    "\n",
    "        self.click_offsets = self.get_click_offsets()\n",
    "        self.session_idx_arr = self.order_session_idx()\n",
    "        \n",
    "    def get_click_offsets(self):\n",
    "        \"\"\"\n",
    "        Return the offsets of the beginning clicks of each session IDs,\n",
    "        where the offset is calculated against the first click of the first session ID.\n",
    "        \"\"\"\n",
    "        offsets = np.zeros(self.df[self.session_key].nunique() + 1, dtype=np.int32)\n",
    "        # group & sort the df by session_key and get the offset values\n",
    "        offsets[1:] = self.df.groupby(self.session_key).size().cumsum()\n",
    "\n",
    "        return offsets\n",
    "\n",
    "    def order_session_idx(self):\n",
    "        \"\"\" Order the session indices \"\"\"\n",
    "        if self.time_sort:\n",
    "            # starting time for each sessions, sorted by session IDs\n",
    "            sessions_start_time = self.df.groupby(self.session_key)[self.time_key].min().values\n",
    "            # order the session indices by session starting times\n",
    "            session_idx_arr = np.argsort(sessions_start_time)\n",
    "        else:\n",
    "            session_idx_arr = np.arange(self.df[self.session_key].nunique())\n",
    "\n",
    "        return session_idx_arr\n",
    "    \n",
    "    def add_item_indices(self, itemmap=None):\n",
    "        \"\"\" \n",
    "        Add item index column named \"item_idx\" to the df\n",
    "        Args:\n",
    "            itemmap (pd.DataFrame): mapping between the item Ids and indices\n",
    "        \"\"\"\n",
    "        if itemmap is None:\n",
    "            item_ids = self.df[self.item_key].unique()  # unique item ids\n",
    "            item2idx = pd.Series(data=np.arange(len(item_ids)),\n",
    "                                 index=item_ids)\n",
    "            itemmap = pd.DataFrame({self.item_key:item_ids,\n",
    "                                   'item_idx':item2idx[item_ids].values})\n",
    "        \n",
    "        self.itemmap = itemmap\n",
    "        self.df = pd.merge(self.df, self.itemmap, on=self.item_key, how='inner')\n",
    "        \n",
    "    @property    \n",
    "    def items(self):\n",
    "        return self.itemmap.NAVIGATIONCODE.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-slovakia",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-22T20:15:19.497046Z",
     "start_time": "2021-06-22T20:14:59.621Z"
    }
   },
   "outputs": [],
   "source": [
    "class SessionDataLoader:\n",
    "    \"\"\"Credit to yhs-968/pyGRU4REC.\"\"\"    \n",
    "    def __init__(self, dataset, batch_size=32):\n",
    "        \"\"\"\n",
    "        A class for creating session-parallel mini-batches.\n",
    "        Args:\n",
    "            dataset (SessionDataset): the session dataset to generate the batches from\n",
    "            batch_size (int): size of the batch\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.done_sessions_counter = 0\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\" Returns the iterator for producing session-parallel training mini-batches.\n",
    "        Yields:\n",
    "            input (B,):  Item indices that will be encoded as one-hot vectors later.\n",
    "            target (B,): a Variable that stores the target item indices\n",
    "            masks: Numpy array indicating the positions of the sessions to be terminated\n",
    "        \"\"\"\n",
    "\n",
    "        df = self.dataset.df\n",
    "        session_key = 'session_id'\n",
    "        item_key = 'song_id'\n",
    "        time_key = 'ts'\n",
    "        self.n_items = df[item_key].nunique()+1\n",
    "        click_offsets = self.dataset.click_offsets\n",
    "        session_idx_arr = self.dataset.session_idx_arr\n",
    "\n",
    "        iters = np.arange(self.batch_size)\n",
    "        maxiter = iters.max()\n",
    "        start = click_offsets[session_idx_arr[iters]]\n",
    "        end = click_offsets[session_idx_arr[iters] + 1]\n",
    "        mask = [] # indicator for the sessions to be terminated\n",
    "        finished = False        \n",
    "\n",
    "        while not finished:\n",
    "            minlen = (end - start).min()\n",
    "            # Item indices (for embedding) for clicks where the first sessions start\n",
    "            #print(df.TIMESTAMP.values[start])\n",
    "            idx_target = df.item_idx.values[start]\n",
    "           \n",
    "            for i in range(minlen - 1):\n",
    "                # Build inputs & targets\n",
    "                idx_input = idx_target\n",
    "                idx_target = df.item_idx.values[start + i + 1]               \n",
    "                inp = idx_input\n",
    "                target = idx_target\n",
    "\n",
    "                yield inp, target, mask\n",
    "                \n",
    "            # click indices where a particular session meets second-to-last element\n",
    "            start = start + (minlen - 1)\n",
    "            # see if how many sessions should terminate\n",
    "            mask = np.arange(len(iters))[(end - start) <= 1]\n",
    "            self.done_sessions_counter = len(mask)\n",
    "            for idx in mask:\n",
    "                maxiter += 1\n",
    "                if maxiter >= len(click_offsets) - 1:\n",
    "                    finished = True\n",
    "                    break\n",
    "                # update the next starting/ending point\n",
    "                iters[idx] = maxiter\n",
    "                start[idx] = click_offsets[session_idx_arr[maxiter]]\n",
    "                end[idx] = click_offsets[session_idx_arr[maxiter] + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-desert",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-22T20:15:19.499041Z",
     "start_time": "2021-06-22T20:15:00.745Z"
    },
    "code_folding": [
     42,
     57,
     91,
     106
    ]
   },
   "outputs": [],
   "source": [
    "class GRU4Rec:\n",
    "\n",
    "    def __init__(self,  epochs = 5,\n",
    "                        batch_size = 32,\n",
    "                        dropout = 0.25,\n",
    "                        learning_rate = 0.001,\n",
    "                        decay=0.0,\n",
    "                        beta_1=0.9,\n",
    "                        beta_2=0.999,\n",
    "                        session_key = 'session_id',\n",
    "                        item_key = 'song_id',\n",
    "                        time_key = 'ts',\n",
    "                        n_samples = -1,\n",
    "                        itemmap = None,\n",
    "                        time_sort = False,\n",
    "                        emb_size = 50,\n",
    "                        hidden_units = 100,\n",
    "                        save_weights = False,\n",
    "                        train_n_items = 96 ):\n",
    "\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = dropout\n",
    "        self.learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.session_key = session_key\n",
    "        self.item_key = item_key\n",
    "        self.time_key = time_key\n",
    "        self.n_samples= n_samples\n",
    "        self.itemmap= itemmap\n",
    "        self.time_sort = time_sort\n",
    "        self.train_n_items = train_n_items\n",
    "        self.emb_size = emb_size\n",
    "        self.hidden_units = hidden_units\n",
    "        self.save_weights = save_weights\n",
    "        self.current_session_id= -1\n",
    "        self.Build_Model()\n",
    "\n",
    "\n",
    "        \n",
    "    def Build_Model(self):   \n",
    "        size = self.emb_size\n",
    "        inputs = Input(batch_shape=(self.batch_size, 1, self.train_n_items))\n",
    "        gru, gru_states = GRU(self.hidden_units, stateful=True, return_state=True)(inputs)\n",
    "        drop2 = Dropout(self.dropout)(gru)\n",
    "        predictions = Dense(self.train_n_items, activation='softmax')(drop2)\n",
    "        self.model = Model(inputs=inputs, outputs=[predictions])\n",
    "        opt = keras.optimizers.Adam(lr=self.learning_rate, beta_1=self.beta_1, beta_2=self.beta_2, decay=self.decay, amsgrad=False)\n",
    "        self.model.compile(loss=categorical_crossentropy, optimizer=opt)\n",
    "        self.model.summary()\n",
    "\n",
    "    \n",
    "    def get_states(self):\n",
    "        return [K.get_value(s) for s,_ in self.model.state_updates]\n",
    "    \n",
    "    def fit(self,train_data=None):\n",
    "\n",
    "        train_samples_qty = len(train_data[self.session_key].unique() )+1\n",
    "        print('Fitting the model...')\n",
    "        self.train_dataset = SessionDataset(train_data)\n",
    "        model_to_train = self.model\n",
    "        batch_size = self.batch_size\n",
    "        for epoch in range(1, self.epochs+1):\n",
    "            with tqdm(total=train_samples_qty) as pbar:\n",
    "                loader = SessionDataLoader(self.train_dataset, batch_size=self.batch_size)\n",
    "                for feat, target, mask in loader:\n",
    "                    \n",
    "                    real_mask = np.ones((self.batch_size, 1))\n",
    "                    for elt in mask:\n",
    "                        real_mask[elt, :] = 0\n",
    "\n",
    "                    hidden_states = self.get_states()[0]\n",
    "                    hidden_states = np.multiply(real_mask, hidden_states)\n",
    "                    hidden_states = np.array(hidden_states, dtype=np.float32)\n",
    "                    self.model.layers[1].reset_states(hidden_states)\n",
    "\n",
    "                    input_oh = to_categorical(feat, num_classes=loader.n_items) \n",
    "                    input_oh = np.expand_dims(input_oh, axis=1)\n",
    "                    target_oh = to_categorical(target, num_classes=loader.n_items)\n",
    "                   \n",
    "                    tr_loss = self.model.train_on_batch(input_oh, target_oh)\n",
    "                    \n",
    "                    pbar.set_description(\"Epoch {0}. Loss: {1:.5f}\".format(epoch, tr_loss))\n",
    "                    pbar.update(loader.done_sessions_counter)\n",
    "                \n",
    "            if self.save_weights:\n",
    "                print(\"Saving weights...\")\n",
    "                self.model.save('./GRU4REC_{}.h5'.format(epoch))\n",
    "                \n",
    "    def predict_next(self, session_id, item):\n",
    "\n",
    "        if session_id != self.current_session_id : \n",
    "            self.model.reset_states()\n",
    "            self.current_session_id = session_id\n",
    "\n",
    "        feat = np.zeros((self.batch_size,1))\n",
    "        feat[0] = item\n",
    "\n",
    "        input_oh  = to_categorical(feat,  num_classes=self.train_n_items) \n",
    "        input_oh = np.expand_dims(input_oh, axis=1)\n",
    "        preds = self.model.predict(input_oh, batch_size=self.batch_size)\n",
    "        itemid = preds.argsort()[0][::-1]\n",
    "        return pd.DataFrame(data=preds[0][itemid], index=itemid)\n",
    "        \n",
    "    def Evaluate(self,test_data=None ,recall_k=5, mrr_k=5):\n",
    "        train_generator_map = self.train_dataset.itemmap\n",
    "        test_dataset = SessionDataset(test_data, itemmap=train_generator_map)\n",
    "        test_generator = SessionDataLoader(test_dataset, batch_size=self.batch_size)\n",
    "\n",
    "        n = 0\n",
    "        rec_sum = 0\n",
    "        mrr_sum = 0\n",
    "\n",
    "        print(\"Evaluating model...\")\n",
    "        for feat, label, mask in test_generator:\n",
    "\n",
    "            target_oh = to_categorical(label, num_classes=self.train_n_items)\n",
    "            input_oh  = to_categorical(feat,  num_classes=self.train_n_items) \n",
    "            input_oh = np.expand_dims(input_oh, axis=1)\n",
    "            \n",
    "            pred = self.model.predict(input_oh, batch_size=self.batch_size)\n",
    "\n",
    "            for row_idx in range(feat.shape[0]):\n",
    "                pred_row = pred[row_idx] \n",
    "                label_row = target_oh[row_idx]\n",
    "\n",
    "                rec_idx =  pred_row.argsort()[-recall_k:][::-1]\n",
    "                mrr_idx =  pred_row.argsort()[-mrr_k:][::-1]\n",
    "                tru_idx = label_row.argsort()[-1:][::-1]\n",
    "\n",
    "                n += 1\n",
    "\n",
    "                if tru_idx[0] in rec_idx:\n",
    "                    rec_sum += 1\n",
    "\n",
    "                if tru_idx[0] in mrr_idx:\n",
    "                    mrr_sum += 1/int((np.where(mrr_idx == tru_idx[0])[0]+1))\n",
    "\n",
    "        recall = rec_sum/n\n",
    "        mrr = mrr_sum/n\n",
    "        return (recall, recall_k), (mrr, mrr_k)\n",
    "\n",
    "    def save(self,name='GRU4REC_NEW'):\n",
    "        self.model.save('./'+name+'.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "afraid-closure",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T22:57:08.995993Z",
     "start_time": "2021-06-23T22:57:08.933688Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "secondary-rocket",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T22:57:09.779381Z",
     "start_time": "2021-06-23T22:57:09.285754Z"
    }
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "interior-airplane",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T22:57:10.261962Z",
     "start_time": "2021-06-23T22:57:10.256974Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_dataframe(df):\n",
    "    df = df.assign(sequence=df.sequence.str.strip('[]').str.split(','))\n",
    "    df = df.explode('sequence')\n",
    "    df['sequence'] = df.sequence.apply(lambda st: re.sub(r'\\W+', '', st))\n",
    "    df['sequence'] = df['sequence'].astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "retained-bacon",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T22:57:25.394018Z",
     "start_time": "2021-06-23T22:57:21.910371Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preprocess_dataframe(test).to_csv('test.csv', index=False)\n",
    "preprocess_dataframe(train).to_csv('train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "rolled-cornwall",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T22:47:56.564649Z",
     "start_time": "2021-06-23T22:47:56.479396Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>ts</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>223</td>\n",
       "      <td>['3772',  '3953']</td>\n",
       "      <td>1419418147</td>\n",
       "      <td>15861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>226</td>\n",
       "      <td>['245',  '1271',  '379']</td>\n",
       "      <td>1419433841</td>\n",
       "      <td>15861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>243</td>\n",
       "      <td>['245',  '1197',  '4307',  '3868']</td>\n",
       "      <td>1421674741</td>\n",
       "      <td>15861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>245</td>\n",
       "      <td>['409',  '234',  '2334',  '2431',  '231',  '47...</td>\n",
       "      <td>1421679507</td>\n",
       "      <td>15861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>353</td>\n",
       "      <td>['4255',  '652',  '4256',  '4257',  '4256',  '...</td>\n",
       "      <td>1420927951</td>\n",
       "      <td>4296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48063</th>\n",
       "      <td>2764044</td>\n",
       "      <td>['3051',  '7182',  '310']</td>\n",
       "      <td>1421233642</td>\n",
       "      <td>4503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48064</th>\n",
       "      <td>2764047</td>\n",
       "      <td>['1871',  '1475',  '577',  '3152',  '1075',  '...</td>\n",
       "      <td>1421319245</td>\n",
       "      <td>4503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48065</th>\n",
       "      <td>2764159</td>\n",
       "      <td>['528',  '6475']</td>\n",
       "      <td>1421059220</td>\n",
       "      <td>12934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48066</th>\n",
       "      <td>2764161</td>\n",
       "      <td>['6349',  '2803']</td>\n",
       "      <td>1421141469</td>\n",
       "      <td>12934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48067</th>\n",
       "      <td>2764164</td>\n",
       "      <td>['1485',  '5733',  '1482',  '2445',  '915']</td>\n",
       "      <td>1421430665</td>\n",
       "      <td>12934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48068 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       session_id                                           sequence  \\\n",
       "0             223                                  ['3772',  '3953']   \n",
       "1             226                           ['245',  '1271',  '379']   \n",
       "2             243                 ['245',  '1197',  '4307',  '3868']   \n",
       "3             245  ['409',  '234',  '2334',  '2431',  '231',  '47...   \n",
       "4             353  ['4255',  '652',  '4256',  '4257',  '4256',  '...   \n",
       "...           ...                                                ...   \n",
       "48063     2764044                          ['3051',  '7182',  '310']   \n",
       "48064     2764047  ['1871',  '1475',  '577',  '3152',  '1075',  '...   \n",
       "48065     2764159                                   ['528',  '6475']   \n",
       "48066     2764161                                  ['6349',  '2803']   \n",
       "48067     2764164        ['1485',  '5733',  '1482',  '2445',  '915']   \n",
       "\n",
       "               ts  user_id  \n",
       "0      1419418147    15861  \n",
       "1      1419433841    15861  \n",
       "2      1421674741    15861  \n",
       "3      1421679507    15861  \n",
       "4      1420927951     4296  \n",
       "...           ...      ...  \n",
       "48063  1421233642     4503  \n",
       "48064  1421319245     4503  \n",
       "48065  1421059220    12934  \n",
       "48066  1421141469    12934  \n",
       "48067  1421430665    12934  \n",
       "\n",
       "[48068 rows x 4 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.assign(sequence=train.sequence.str.strip('[]').str.split(','))\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "wired-reporter",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T22:51:32.737692Z",
     "start_time": "2021-06-23T22:51:20.847737Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement re (from versions: none)\n",
      "ERROR: No matching distribution found for re\n",
      "WARNING: You are using pip version 20.2.3; however, version 21.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\ilyes\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "proper-maple",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T22:51:47.207062Z",
     "start_time": "2021-06-23T22:51:46.750121Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        3772\n",
       "0        3953\n",
       "1         245\n",
       "1        1271\n",
       "1         379\n",
       "         ... \n",
       "48067    1485\n",
       "48067    5733\n",
       "48067    1482\n",
       "48067    2445\n",
       "48067     915\n",
       "Name: sequence, Length: 207214, dtype: int32"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "train.explode('sequence').sequence.apply(lambda st: re.sub(r'\\W+', '', st)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-tracy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
